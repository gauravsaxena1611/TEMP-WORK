## Detailed Video Transcript

### Part 1 Transcript (Excerpts from "Part-1.mp4")

up the conference page where we have agenda.

Thank you. Shino, I saw your response for the geni case. Can you please give some notes because this is weird why it's supposed to be on Jen's brag book or something, right? That they want this information for.

I I already sent the knock.

So there are a few responses that are coming in.

Um so I think the one thing that will be pending that will have to come from Mangesh on the usage.

Yeah, that's why I added him. as well because Monges and Shard probably needs to jump in.

Yeah. Yeah. But remaining information is already flowing through.

Yeah. Thank you. It's crazy that why we did not know if it was due 8:00 a.m. this morning.

Okay.

I pinged Chiag already and I see a few other folks join. Hey Shira, thanks for joining.

I have stop at 11:15. So 15 minutes before our target. Okay. So again like today's call would probably be short anyway because our agenda item and and for all of you right who joined we don't send email because email gets lost. We maintain this confluence page and today after this call Gorov will send you guys email with this confluence page link again. I know that we have done it. We'll do it again. So bookmark this somehow because you know sending 10,000 email is not going to help. So we put all the minutes agenda item in one page right. I know that uh last time we we spoke about Nick Hill's item which is how to resolve the new uh UI or the policy uh authoring part that we are trying to do. I don't think we have a solution yet but today's agenda is I know that Dura is out so DA had something for today he's out so we pushed it to next week. Today's agenda we want to talk about briefly what we need to do for the recall. Right. And part of the data requirement, data use cases, we have to do some recon. So let me share my screen or I I'll show you um what I.

I just quick question right. I still don't understand or maybe I'm missing something that where the use case says that there is a need for recall.

This is a this is a requirement. This is what I'm hearing that it is coming from Miran and other folks. So this is what happens. Are you guys able to see my screen?

Yeah.

Okay. So now this is the current use case how it works today. We have all the data source. We have data injection process. It comes to CRC or you know we have our big data environment and from there we push data as SFTP to EDQP platform right and our business users goes there into data catalog. They set the rules and those business this tool flows into EDQP platform. So there's a very high level data flow that happens and EDQP platform has scorecard DQP scorecard dashboard and they also get email alert right so this is today's process for those of those folks who does not know what it is and how it works today so this is how we do our data quality check that has been provided by I think EDO office right we worked with EDO office.

yeahqp is their platform and we are doing this I don't know from how many years now.

right so now what is needed and we need to go from there to this part which means we are the pretty much this part remain as as as usual so what we are planning to do integrate with open metadata to provide the data lineage so open metadata will actually go into data catalog right so I hope that you guys see this arrow so we'll embrace the solutioning that has been pro proposed and approved by Shaji and everyone else. We'll be utilizing open metadata to push all the uh rules etc uh and data lineage and data cataloging to enter data catalog. Okay. So this is one flow. Now the other requirement was where the EDQP uh exception reports or u you know SLA or um you know the scorecard will be shown. So we we got a confirmation from EDQP folks that they are already in uh integration in place that all the scorec card from EDQP goes to rock. So we are covered because that was another requirement. The regulators and others they want to see the scorecard in one place which is rock. They don't want to go anywhere else. So this part we are covered. We got a confirmation. So now the second part chak to your point there is also requirement that we need to have data reconciliation. and and that that does not mean the data reconciliation on the record count uh count of the record they want completeness means like we have to show record by record cell by cell and I you know it is important and that's why I invited Shinu as well to join because for MCA feed if you remember we had this that hard conversation with MCA guys that they're sending us data but how do we do record level uh detail not just row count right.

there we were trying trying to cover our tax ground basically so that we will be covering our ground clear but when it comes to the use cases I'm not clear that where it is coming as a mandate per my knowledge till date there is no such requirement in any of the use case guidelines to have a recon built-in or anything like that.

no it is it has been written so I could show you that email where Mirang from EDO office right works.

that will help us cal it you go to there is a guideline setan correct me if I'm wrong Krishna has shared that also there is a gio guideline shareepoint where for each use case for each milestone and deliverable there are clear guidelines published.

and if it is part of those then it makes sense if it is not there then it is not making sense that we go and build something extra which is not even part of the requirement.

right so if you see.

we have maybe it's an awareness thing that we at least use case leads were not told about this so If there's anything that you have an extra cal just share that across. We will recheck and see if it is really needed to build on our side.

Yeah. So as you see this is what I'm I'm sharing and I want to make sure that we don't get slapped for something that been overlooked. So this and what Shaji made sure wants to make sure that we follow the same strategy for not only erm but also for ICRM right he doesn't want us to reinvent a wheel. So this was part of the erm data tooling right that was presented that At the beginning they said they want everything in the neur. Neur is a system that shows tracking of every single data point not just metadata. Right? So there was a misconception earlier. We thought neur does the reconciliation. Nur is not just a reconiliation. Any data point or CDE item that you define in uh what you call the the platform Olympus neural take those CDE and connects it to all the hops. It shows not just metadata but actual physical data where is it flowing from right so that's what neur does so then we ask like do we really have to use nur because if we want to use nunare we have no other option but to send our data to Olympus which we are not going to do that that has been already made made clear then we asked so can we use our own recon engine and this is the answer that came from and miron is from co office right he works Same person who was on the workshop right reporting to Andreas.

Exactly. Reporting and so they are the team. This is the clarification.

Yeah. Right.

I'll forward this email to all of you. But this is the clarification.

We do not have to send it to Nurk, but we have to make sure that this is covered. I I'll give you guys a minute to read. So it's not just coming out of me or Terry. I just want to make sure you all understand.

Not it's it's not even if it is coming out of you. Also my my simple ask is that which is a fair ask in my view at least you can correct me and and definitely shut me down. There is no harm in saying that also but my point is that because use case is totally governed by the EDO guidelines published on their shareepoint which gets refreshed almost every week right as long as the same guidelines goes in there I'm fine because EDO team is responsible for that if I go and read those guidelines it reflects me that there is a recon Because if you look at from this angle right there are three three controls and we are we are talking about 14 controls from the last three years because we have accomplished many deliverables in the last three years with respect to use cases also where we have talked about 14 controls. Uh Chetan is well aware of that because we were dealing with them in previous RBCMs also. Out of those 14 controls always we have closed the audits also in the last two years. It always comes back to timeliness, completeness and accuracy. Accuracy is something which is not tech can confirm. Accuracy is something which is owned by the business. Timeliness is what we take care. Completeness is what we take care. But our definition of completeness from the tech side of the control is very simple that the data what we got is complete in terms of looking at the control information of the data. So we get data along with the control information which totally relates to the header trailer information number of records and all that stuff. There used to be a control out of 14 controls.

that used to say that comparison between the two repositories.

but tech never owned that control because we are not going back to the same system and going back and checking against and we had a similar discussion and we were trying to do senu you me had a same thing in the MC also where we were trying to build and say that so that we can cover our ground that what we have received is matching many on the record level column level value level everything but tech from the completeness perspective only the header footer you can clearly say all record processed example header footer that is what we are showing to auditors from the last three years.

I know but my point is like for this point the last point.

can you come in and give me a return that you you are doing it on the tech side and covered?

what do you mean uh by the uh data covered means?

means data covered means that role level role level comparable.

right see completeness and consistency is what we We normally show is we have received 100 record we have processed 100 records and 100 records are available now 100 for junk we have processed the junk it's a junk in junk out.

so what I was told uh chir that covers your this part but it does not cover the consistency part means the record level comparison so do we have record level comparison?

this is the completeness consistency is one of the because As part of the milestone 2 there is a control evaluation also right and that is done by our compliance working with us and as part of that we do the control evaluation only that these are the controls which control is applicable to tech which control is applicable to the business and for each control what is the corresponding MCIDs.

can I share a screen Chira I think what you're explaining we are aware of but it will help for the audience if they see a picture and then you continue explanation.

definitely definitely thank you Please please.

okay.

and again for for folks these are design discussion so we are talking about just the base point now and we'll go into deeper dive in few minutes of what can be done yeah.

yeah are you able to see the screen?

yeah.

yeah so these are at the ch I'll let you continue just giving a high level background at the bottom you see these are data quality controls based on the new CDGP principles there are 10 controls that every process has to be checked checked on. Some of these are tech own, some of these are business owned. Uh on the top you see a data flow diagram that is a target state for a single use case uh which is customer complaints and we can go into specific as to what is really applicable for consumer. Now within ICR and consumer what is applicable for a technology? What is applicable for a business and we address only those controls that are in scope for consumer technology in this case IC Jira go ahead.

Yeah. So when it comes to the control assessment, we have one of the deliverable under the milestone two of each use cases which says that do the control assessment and during that control assessment is what compliance work with us and understand it used to be 14 controls. Now with the new edom principle only 10 controls are there out of 10 controls what is applicable to tech is what get documented and published to the closure rapper goes to the auditor. If you look into these 10 controls, first one is annual review of the enterprise data and data catalog. That is not tech control. We we don't do the annual review of data catalog. It is owned by the business from the data domain.

It is I'm not talking about data catalog. I'm talking about the.

I'm going into each control.

Okay. Okay.

I'm going into each control. Second control review and validation of data aggregation requirement. Again, not attack control. It is again the data which is business own. Third one internal data cap. capture, injection and creation. If there is any such process being part of the development where any data capture, injection or creation is part of the screen or anything applicable. It is conditional based on that applicable not applicable depend on what process we are building and everything like same is aligned to the next one also that external data capture and injection. If there is any external data coming from outside system which is where it has to be captured and injection you just need to reflect that control. Next is the monthly data exception management. If there is any data exception principle in place that on a monthly basis you are doing a validation and you are raising an exception which aligns with your data concern which is the next concern. Six control data concern management it aligns with fifth and sixth goes hand in hand with that perspective. Seventh one if you see completeness and timeliness of the data transfer and if you go and read the definition from the EDI principle of it this completeness and timeliness only talks about that you got with respect to the control information, timeliness with respect to the SLA that you got within the timeliness and everything and rest is review of authority designation quality data quality review time bound plan to achieve CDGB and a compliance is not tack on controls. Seven is you were doing the microscopic view of seven actually which is the core tech control in this area.

Yep. Yep. So long story short I will get a written confirmation that they need a role label.

That is one but also what is it? Why is it coming across as a need? What is that control? What is that CDGP governance or policy that is asking them to provide a recon evidence? That is what we should drive the conversation.

I I think Shinu has a question but before we do that guys we'll go all over the place and spend another 10 days to 10 weeks on this.

My point is like what is stopping us to use the recon and do the recon? That's my question to ask.

Absolutely. If it is needed, we will go ahead and do it. We're just asking because data use case leads we haven't heard that expectation as such. We would like to know more so that we do it the right way.

It's not it's not about that what is the concern why we cannot do it. It's all about because these are consent total deliverables.

Doing something extra does not harm. I agree. But we need to abide by all the deliverables, the documentation, everything for the audit tomorrow. So that is where I'm little little I would hesitating in proceeding as long as the guidelines says and we go and that's why I was I was saying earlier if you go and talk to those people if you talk about everyone will say yeah yeah we need this that's why I I am requesting only let's look at the guidelines published on the shareepoint first let's do a deep dive on the guidelines then have a connect with the edio people if that comes to be an agreement then it is applicable to all use cases anyway there is no choice we all need to do it.

there is no choice then yeah.

to come from that channel that's what I'm requesting.

Shinu something please.

yes so on the seven right number seven so it is clearly not documented anything related to the accuracy check it is talking about only validate completeness and timeliness of the data transfer that is all it is talking it is not talking about accuracy so let us validate that one whether accuracy is part of that number seven or not that's it as per at least this PPD is concerned it is talking about only the completeness and timeliness.

So which could be that okay a feed is coming in by 7 7 p.m. and it needs to be processed by 9:00 p.m. that's our SLA and if that is the timeliness that we are expecting we have to make sure that okay it is processed as per that timelines that we expect right and similarly the completeness there is a control record as as long as the control records are validated after the processing of the data that will cover the completeness acc is something even not even being asked here. So from the data cons data standpoint it may it may be uh documented that this is it right but cal from your standpoint accuracy thing and then the connection of the accuracy check that we are doing for CRA I just want to bring up a point so that CRA thing is actually a businessowned control okay it is specific to CRA The reason is this because CRA is not a continuous process for the entire risk assessment cycle which is annual basis. They need to utilize the data as of a particular snapshot of time and they use it for the entire year. So if the accuracy is basically not verified at the beginning of the risk assessment cycle they have an impact. That's why business was owning that control and it is that control was documented in MCA. And the verification of that one is also done by the business. So the way that they would do is they bring the data from the MCA in a form of the Tableau report and then they bring the data from the CRA and then they compare the data by themselves to confirm that accuracy of the data right and we we kind of put a systematic control that's a business control right we try to put a systematic control when the data is brought in such that we can basically um um identify that problem sooner than later. Like after business going through so much of exercise and then they find some some issue, can we not find that issue sooner systematically? Right? Because the business process is going to take long time. They would take like 3 to 4 weeks. But after 3 to four weeks, we come to a scenario where they've identified an issue. Fixing and rerunning the batches all that would be additional time. So we we are losing time there. So in order to not to lose the time we put that systematic control but we may not want to generalize that one for the entire data processing. Okay. Let us validate that one before uh we proceed with anything.

Right? So we'll get that validation and written note. But my question for all of us being proactively looking into how the recon engine works in erm area and they already clarified to us in in the demo that they did and we could ask additional time for the team that how can we use recon engine on our side they're happy to give us the library so we don't have to move our data from our CRC to erdl or erm we could bring their library and run it on our own infra that does the recon between data source and what we have in CRC right?

question there yeah question there. So is this recon engine is it something internally implemented or is is this the neur?

No this is internally implemented. So initially it was asked that we need to use neur but then we got a confirmation from edo team that we don't have to use nur if we could show those three point that he gave us on the email and this is internally built engine right so and again like being from the technical side Chira I know we did not had any issue we did not had any control in place but in general I I know that we are in time crunch but as a data consumer side side right there is no harm on utilizing or looking into the recon engine because we might uncover something or we might actually improve our process by doing the recon ahead proactively.

There there are two points right it's not about start adopting something we are happy to adopt anything which makes sense and and benefits all of us right but I'm going from the guidelines specifically when it is a consent order right we are already dealing with something which is a very time crunch and bringing something on our own which is not part of the guidelines will impact us somewhere in terms of timelines if you want to do it later in the BAU yes by our way anywhere we all will accept and proceed with it but introducing something which is not asking us from the guidelines perspective and then when we go and talk about it with respect to auditor and everything it might benefit might not benefit we don't know how auditor will take it and take it forward we don't know at that time but when it comes to erm let me just add so that audience know right what it is because yes I have seen what erm showed us earlier also erm has a specific need for it there is a use case which which they want to implement ment it they have to implement it there is there is no exception for them why because erm is becoming an AR for the data when we look into all of our compliance use cases we are not the AR we are the end consumer of it so end consumer has at some set of controls where we will be applicable to and we need to go and abide by those controls and rules when it comes to a system like AR means I need to go with something ext RA because I'm distributing that data further to somebody and when I'm distributing that data to something I need to go and abide by many controls which is part of the data governance policy also which is the DCRM process also need to be followed data quality need to be followed lineage and everything you need to be extra careful because now you are the distributor of the data so erm and specifically when we look at in the recon what they are getting they are getting the aggregated numbers right they are getting the final aggregated metrics from each risk stripes whether compliance is one of them. So what we are doing is we are giving them an aggregated number they are picking the aggregated number comparing values by values 100 records or thousand record or 5,000 records or anything like that when we look into from compliance we are the end consumer of the data we are not the redistributor of the data further right so when we read that data what we received let's say Olympus as an example Olympus is an AR Olympus has to go by all these rules they have to implement neural or whatever they want to do it when we received data from uh Olympus, we received 100,000 records. Now what those 100,000 records if I have to build a separate recone where that recone engine will also read those 100,000 records from Olympus and compare and say that means we are doing above and beyond which is not something warranted or guaranteed from the use case deliverable perspective or from us as a consumer of data perspective because we are going with only timeliness and completeness.

Okay,

accuracy was not a part of tech control anytime. Historically in my last 15 year of dealing with the data in this form at least.

accuracy is always maintained by even we had the same challenge in BACOM 550 two years back in RECW where we had specifically out of 14 control. This was one of the control clearly written comparison between two repositories and how we close the audit on that. Okay users we are pulling the data we are showing the data on our screen users need to go and validate in wherever the screens of the source system and validate that yes this is what we are getting it from there because that's how users need to validate those control not the attacks.

so don't take me wrong on that front I'm not saying that we will not but I'm more concerned is keeping in the scope of use case primarily if it is not guaranteed by those guidelines actually.

yeah so I'll get that confirmation and that's a valid point um chiak I I I don't you know with the time like do we really have the time to embe em embrace uh this recon engine But nevertheless, we should probably look into that and see proactively how can we do that to Shinu's point for CR case and other use cases right and I will set up a separate demo to take a deeper dive because that day I don't think we had enough time to take a deeper demo of how the recon engine works from erm and how can we really port it and bring it on our side so we don't have to reinvent the wheel right for for MCA we want to be proactive um now So the second question that I have not not second a few and mid I'll get to you in in the in a minute the question that I have for you the report the completeness report uh the exception report if and SLA report all those thing are already taken care by EDQP people platform am I right I think I'm right but I just want to make sure that this EDQ people platform from who you worked with before these reports are already been generated in their end when we send data because they also get EDQP also goes to data source right to get the data for data quality check is that a right understanding or no?

should I give froze?

I'm trying to think can can you please repeat that once for me?

the the question is like you mentioned that we do the completeness.

completeness means the row count that we get from data source and on our side is correct. Who is responsible? Is that do you have a process that uh creates the report and inserts data into database or you create a report for yourself on a dashboard or EDK is responsible for?

as part of the data governance. What we normally do is for every this this rule came in around three or four years back before we even implemented the autoatch plush where it was forced to everybody in the bank that wherever you are invoking a fee or sending a feed, make sure every feed has a control information. If your feed does not have a control information, then at least have a separate control file to support that evidence and we use that control information whether it is a part of the same feed as a header trailer information or it is a part of the separate control file because we do get a separate control file also in terms of LMS as an example L which is L nowadays and GDW as an example GDW standard feeds but they give you the control file so we validate against that control file or the control information available within the file and then with that comparison we produce that okay this control information and the control file says that they have sent you thousand records and we have received a thousand record in our file we evidence that in our process of build when we go and look into the logs on our server our logs will be printing it clearly that okay header validation started this is the number of records and everything when the feed processing is happening and we evidence that to the audit every time whenever they ask for it that Yes, this is clearly visible in our report and logs that yes this validation was passed and it is clear and then we generate one email also at the end which shows that number of record received number of record process any rejection or failure also gets reflected in that email. So that is our.

so we have that covered on our side from that's what we are documenting and sharing with audit from the last two three years across various RBCM whether it is a CIA CRA or any of any where we process the data from outside.

### Part 2 Transcript (Excerpts from "Part-2.mp4")

This is clearly visible in our report and logs that yes this validation was passed and it is clear and then we generate one email also at the end which shows that number of record received number of record process any rejection or failure also gets reflected in that email. So that is our.

so we have that covered on our side from the that's what we are documenting and sharing with audit from the last two three years across various RBCM whether it is a CIA CRA or any of anywhere we posess the data from outside And and second part is the exception management right for data quality check that is been handled by EDQP right they they they do the exception management and they send the alert right.

yeah we are not responsible for that anyways.

okay good so only part that we need to cover we have completeness we have our SLA both been covered in the through our process and documented through email and via uh uh logs etc. Do you have any dashboard that shows uh or report that shows uh angular report that shows that Jira?

we we do not have but as part of the com 80 implementation two years back somehow audit pushed our team and Mangesh and we end up building one tableau screen where we started showing that for com 80 two three years back and.

correct there is a report and a screen to suit it.

So that was somehow we we never wanted that to be because that is not a control as such but audit somehow because it was a challenging discussion format. After one year of implementation, we were trying to close the audit and finally Mangesh and team we had to build immediately one just Tableau screen uh tableau and we are pulling and showing that okay this is look at it same thing we got pushed by then uh in the 550 also and in 550 then we made that as a part of our Tableau report itself that whatever feeds are coming on a daily basis we are showing it in our Tableau also but it is not a general guidelines it is not a general rule that everybody need to implement this for all the places.

but we have this in place what I'm saying those timeliness and and completeness we have it in place in the.

report evidences but you you are not by that you need to go and create a report for every such case and show it to everybody now.

okay m you had question sorry I'm.

yeah no I just just want to bring in another business context the conversion of recon uh and I I I kind of extend the same thought process which mentioned but Just to give from another perspective the perspective is from the compliance business function point of view the key the key ask from at a higher level is it's data provisioning. So basically the second line of defense which is ICRM wants to look at the frontline data and to look into the data to to identify any potential regulatory violations or potential policy breaches. So that is the end purpose of all the data we're doing in compliance data use cases right as opposed to so with us when completeness is what is required and even second line even if you build the tools the data owner only can confirm the accuracy and ICRM second line is not the data owner the front line is the data owner so even from the process point of view it's a it's a not an appropriate task so what compl has to do is give me the front line data and look into it. Find out potential relative violations and policy pitches. That's it. The front end gives us garbage data. Look at the garbage data. Front end gives us good data. Look at the good data. Right? The data accuracy responsibility is the front line. Now the second point is it makes more sense for processes like fin in finance we have general booking. So we have set up transactions and we need to apply those transactions to general posting to create the balances and then that goes into the financial reports, payroll reports and the There we need the accuracy. There we are saying this is our PLM number, these are revenues, these are expenses, this much you pay in taxes. That has to be accurate. So the me you went on mute.

He dropped I guess.

I think suddenly there was an alert from Zoom. At least I got one.

Yeah, I got one. Yes. Zoom. AI has taken over. He's done something. I guess he clicked somewhere or what?

No. May spoke something wrong and then AI kicked him out.

Yeah. Did you hear me or not?

We didn't hear the word that you may have used because got thrown out.

No. Did you hear me completely? Okay. All I was trying to say is that.

But yeah, I heard you.

All I was trying to say is that from business process perspective, right?

He's serious. Okay. Did you hear me or not? Should I repeat?

No, please repeat. Go.

Okay. Did you hear? Did you hear what I said or No?

No. No. Please go ahead. Repeat.

Okay. Okay. So, what I was trying to say is that from I just want to extend what Senu and uh no, we heard all that stuff like in the end right when you talking about financial ledger.

Oh, that's lost. Okay. Okay. Sorry. My bad. I didn't know where you lo where I lost it. So, basically I saying that in in financial reporting that has to accur like how much revenues we made, what is our losses, what is our P&L and what's our liquidity that has to be accurate. So those like accuracy applies on financial risk taxonomy not for non-financial risk taxonomy which is second line.

Okay. So given all those point and those are all valid point right my impression here is not really business we need to I think there is I'll get the clarity that if we really need the completeness check But made me to to the point is not about I'm getting data from front line or someone else. The point here the recon engine purpose here we get data from any source we bring in for our processing and we have our own data layer. We need to have a recon process to make sure I'm getting all the data correctly from the data source. So one example here and again I'm bringing the MCA issue. We are getting millions of record fine we do the record count million it matches SLA matches in that millions of record do we know every record in that million data set is been populated on our side properly or not and sadly I don't think we have an answer right now because we are trying to get MCA team to work with us to give us a check sum that every record is been persisted on our side properly yeah so m that's the point we're not talking about the fin financial requirement. We're not talking about you know who validates the data. We're talking about just a systematic check proactively make sure that we have a complete set of data. Something did as a process failure or hiccup did not get messed up in between in record level. Yeah.

Yeah. I I I I have something to talk on on Mir but I'll speak after uh Fache.

Oh yeah. Fash has something please.

Yes. So K whatever you are bringing up right and whatever churag explained right that's the basic of all the ETL jobs that we have we have a control file we have a check sum we have an auto wash to monitor the feeds everything is been taken care as part of that okay now I agree with me on on the points that I wanted to talk about completeness and timelines even though you may get a proper check sum you may have a proper control file b on the days no stale data whatsoever we still may not be able to do the data quality check 100% because that's the data quality check the business has to review the data and then basically make sure the data is complete in all respects and it it basically uh bas whatever they're looking for in terms of reporting and then pre uh post-processing after they we feed the data back to them that has been taken care that's the that's the place where your actual data quality comes into uh review and only After that part is done and you can basically say yes the data is complete in all respects from the loading perspective if you have a proper data values yeah you can build some exceptions for example for on the date uh for example just to take an example on a date thing or if you have an SOD column or you have a proper formatting and all those things some of these things you can do but if you are getting hundreds and thousands of fields in a data file and if you don't have proper business rules it is not possible to validate 100% of the file on the techn technology side to make sure that yes our data is complete in all respects.

Yeah. Now up here the co team me and others they're not talking about data quality. They're talking about.

completeness and consistency.

Yeah.

Right. They're not asking for data quality. So this is just a recall. Go ahead. You had something else.

Yeah. So I mean to me's point right. So in the financial ledgers and all that stuff like it's very important but may to your point where the data is is actually owned by the front line and then ICRM is going to review whatever the data that is given by the front line that is correct. Agree with you but thing is that there is no guarantee that systematically something is wrong gone wrong right front line may be giving you the right data but when bringing the data into the ICRM maybe something went wrong and what they're reviewing is actually the wrong data what ICM is reviewing is wrong data right so it's not garbage garbage in it's not it's not garbage in it's the right one but garbage out because something went wrong right so it's not it's not about just the completeness even all the CDEs for example um there are thousand fields that we are consuming it but maybe 50 fields are critical data elements right at least those 50 data elements are expected to be validated to ensure that we transferred the data from the frontline systems into the ICRM accurately.

but that is called No, no, no. Quality is different. Let us say if the business the frontline system itself is having supposed to be an X value but actually they had a Y value, we consumed the Y value. The Y itself is wrong all throughout. That's a quality problem.

Correct.

But we not talking.

once they give you once they give.

they give the Y we got the Y.

Yeah.

That systematically is correct.

Yeah.

But we want to prove that.

No. If you got a Y instead of an X who will need because see for me if I'm getting in a certain field it is okay right but you as a business or somebody else is a business they know that okay instead of Y I should have received an X so that's a quality issue.

correct that's that that's what I'm saying if the source system itself is wrong.

we cannot do that right.

no we not talk about is wrong.

then we cannot do when we cannot do.

that's a business thing right.

yeah data quality would be handled authoritative distri redistributor or the system of record whatever the value that they're giving we consumed that value if that value is being wrong that business will have to find that out okay if data quality is here we're talking about here we're not touching data quality uh for now we're talking about completeness like all the data that comes from here and all the data is in our system are equated we need a third entity who does that validation check.

so basically When the when the data is at rest is okay. When the data is in movement, we don't want to lose. If we receive why, we need to show why.

See, it's a good thing. See, I I implement we we went through this one in CRA. I think it's a good thing to implement in the system. But it's not an easy thing. It's a complex thing.

When we have to implement this across all our data processing, it's very important for us to validate how much mandated it is. Right? If it is mandated, then we will find a solution and and then and then implement it across right um if it is not mandated it's a complex complex there is our our our whatever that we are actually planning to do our deliverables they will all get impacted you you you will have a lot of effort that you might have to spend.

yeah so that's the point we're trying to make right I'll set up another demo hopefully sometime next week to review the recon engine how it works in in action because I think last week we did not get the full uh full picture yet. On the side note, I will get a clarification from Miran that for our data use cases, do we really have to prove the consistency? Yeah, completeness, timeliness, we got it. Is that sufficient or we also have to do in my opinion? Probably yes. But I'll get that written confirmation. But but also the volume is important right complaints has a very very high volume and it it has got all kinds of complaints. So I think for complaints it will not be MC is still a small population.

but again like I'm not going to take it on me I'll let them speak for themselves.

Yeah I'll tell you one practical scenario that happened right the the data check the accuracy check that we have done it's probably like almost 100% it will be fine. As long as I mean things are implemented pretty systematically it'll be fine. Whatever we consume we would have dumped the data into our tables. We rarely identified any issue with that. What but still a business has identified there are problems right that is the real problems right? So the control that that business has put together where accuracy check we took whatever the data there is in NCA we brought the data over and Then when they try to reconcile it, they have identified some scenarios that um some of the third party mappings were not right to a control right so that's because MCA itself was not giving some data to us but that they were able to identify that right they did not give so we did not consume but business was able to identify that one through some other means of doing the reconciliation So that's a true data quality issue.

Chak you had something else.

Yeah. No, the same point uh uh Senu has nicely articulated that and given a need also if it is. Yes. It is not something but E2 recon there used to be a team in the past which was moved under EDU. I'm talking about six seven eight years backtory. Okay. It is not a new in the firm also but there the process was needed. because the compliance was auditing the AML work which is the AML mention screening and case generation and monitoring system because I used to support that 78 years back. I was part of the CPD uh AML from the GFT standpoint at that time this group was under EDO those days and it got separated and it used to a separate EDO team which was responsible for E2 recon that team and project itself named as E2E recon I hope it still exist. And their role was simple. Whatever data from any place going to the AML work for the transaction monitoring or AML, they get the same copy of the data and then they get the same copy from them and do a comparison and produce a report because it was mandated from the audit point of view that you are auditing the second line but you that function cannot exist in the same domain. So I cannot test a test myself and say that yes I am good. So So that's why that group was separated and it was kept in the EDO as a separate function to validate the work which is done in the EML for the same end to end recon perspective and I hope EDIO still has that team and that operation is still in progress.

So I also want to give you guys a bit of overview of what I'm going through with IDOT and getting some clarification regarding sharing data right if we have multiple systems who sends us data and again like I'm coming back to our MCS in ADO right what we need to do so let me know if you guys are seeing my screen so there is a new concept called licensed application are you guys familiar with that.

I'm trying to get a get a clarity.

yes I'm trying to get a clarity of licensed application And what we need to do and this is the point where Chira you are mentioning that being an AR being an AR is is pain in the butt because you have to do so many things and if you are distributing so the argument that I'm getting and I'm trying to fight off but that has a good reason why we need that recall. So let's say we have one process called data proxy and you know sorry I don't mean to bring up but it is a good point right data proxy gets data and it sends data to five other processes. So if data proxy is wrong when it gets data from MCA all five process would be wrong. So that's why they're putting in all those control in place that if you are distributing regardless authorized distributor or LA you need to follow those processes even if for temporary. So I'm trying to fight up that hey I'm just getting data I'm not persisting it's on my cache ephemeral state why do I have to do that and this is what the licensed application licensed application what they're saying let's say you have a capka queue or capka broker been provisioned who does not persist data into physical database it just holds for temporary space and gives the other system then it could be a licensed application but I'm fighting or arguing with them kapka persist kafka also has hardware why it persist something could go wrong so why licensed application cannot hold in the temporary database. So to me persisting into database and persisting into Kapka Q similar but it will give us a lot of relief if we could use our data proxy for example as a license application versus AR because AR has a lot more responsibility like what you mentioned Jira that ERDL is trying to be AR because they have that responsibility and that's what also Shaji wants he's like I want data to get once and be distributed among ER system not erm er means under everything in Charlie we get data for IAP we should be able to distribute and I'm trying to find a hack I wouldn't call it hack find a way without being an AR because license application we can do that as well so it is it is a complex decision and I'm going back and forth with team they are you know running away from me but I'm trying to get them to answer my question that can I use data cache would that be in memory cache? Would that be just a temporary database like Oracle or Mongo and be a licensed application? Because we could blow up that database or that cache at any time, no harm, and we could get the data back again whenever we need right. So this is what I'm trying to get a clarity on that if we get something from SO can we become just LA licensed application and distribute among both system.

I just want to add a few things K if you're done.

Yeah, go ahead.

So as as per the uh data governance policy or the CDGP guidelines, right? The CDGP guidelines clearly says that if you are the AR of any particular data, there are particular set of controls and data quality and all that governance need to be placed. I will give you a specific example in this case which I spoke in my morning dev status call about REGGW because RegW is getting penalized for becoming a source of data for one of the finance select reporting which is a daily report goes to the regulators from finance. So right now in that discovery for one of the CDE which is the total covered amount we are becoming the AR or SOR basically not even the AR and parally I'm working with the CDGP team to see that there is a use case one where all this being tracked and we need to go with certain controls and certain from the data govern principle data governance principle point of view to implement those one is very simple is register those CDs make them in the ICM domain that is very first thing and then what controls so all the data quality rules and everything go and implement that quickly second thing is now measurement of those controls okay you have defined the CDs you have defined the data quality rule now you have to make your designation in the data governance uh tool itself the uh our data catalog tool you have to go and yourself that I am the SOR for this particular CD and for this particular CD I am the AR where I am the AR I need to go to upstream also.

and define that relation and quality control measurement from the upstream also where I am the SOR I need to go and do the now data quality measurement for that I need to work with ECDO and give them data whatever they need so that they can do the data quality measurement and that scorecard evaluation on a daily basis there are some tech control I'm already dealing with this right now just because of that conversation and I'm already implementing all these things right now because ads is getting applied to the ragw right now.

so it is it is it is a complex thing right and and I if you see that there is a right reason why we have those in place and there is a right reason why we need the recon uh as an independent uh completeness check and I would not do ask anyone to do the demo today because a lot of other folks did not join and again this Friday's call I'm going to probably move it to uh either Wednesday or Thursday when you guys most of you are available because I I realize that for India folks it's very late right and I already made that point uh in a games meeting earlier most of you are there uh we'll do a demo so shinu let's take a deeper dive and understand how that recon uh thing works so that way we will have that entity so if we have to go and define ourself an AR we will define AR because that's That's Shaz ask that if we get data for someone outside we should be able to redistribute oursel we don't need to persist right uh not because we don't need 10 different pipes to get the same data from the same data source if we have 10 process so how do we get around so I'm fighting up with idio team to publish our process I did not name any process but process like uh you know data proxy to become a licensed application which is lightweight compared to AR.

Yeah. So I think see it's at the end of the day when it comes to the accuracy checks if that is a business owned responsibility where they compare the data with MCA and uh and systems like CIA right let let's take CIA CA would be good example here right so we bring the data from the data proxy uh from the CA into the data proxy and we kind of like a distributor from there to the CIA. Right?

So now CIA is not directly consuming from MCA but they are consuming it from this internal distributor. Right? So.

now CIA users have the responsibility to confirm that they're able to get the controls whatever are defined in the MC.

Mhm.

Right. We we expect them to do the UAT signoffs on those things right and then when when they go to the production they do at least a sanity in the production and then release it to the users right so that's the minimum expectation in this one right in this process it is only that instead of everybody developing the consumption of the MCA we wrote as as a good engineering practice like okay we brought that data in then we basically shared but that doesn't mean that business will b basically skip the validating for their control control validations, right? They still have to do it.

but this is just our internal implementation right?

so I may move this data into maybe something like an elastic for example right after moving that elastic it the data is basically still read from multiple applications are reading from that elastic it's not that okay again we need to do some accuracy checks right ultimately the check needs to be done is Whatever data that is there in MCA is the data that they're seeing it in CIA. That is what is the check but that check is owned by business.

Yeah.

It doesn't matter what is what is internally implemented, right?

Y yeah.

So we don't want to do the can of worms, right? Unnecessarily bringing up like it's a complexity. There is it is going to add up a lot of complexity in doing this stuff. We have to do it for every data feed.

Yeah, it and and my point is like there is a control for good reason but overutilizing is difficult. So.

we we don't want to use that but you know we also have as a good citizen we have to do that good practices to do that you know external recon to make sure systematically we are covered and I don't think it's super difficult to do if we already have processes in place right and.

you saw how difficult it was like because source system does not give a check.

exactly they don't want to.

it's difficult if at all you don't have a systems that can actually talk to each other in a consistent form. You don't have you cannot bring the consistency.

because they cannot consistently talk to each other.

So want to add one one point to what J just said, right? We had a similar thing implemented DSMT a couple of years back because I was owning it. GDW data I am owning it still today I'm owning it and it is used across if you look at not only within compliance it is used across GRCD and now outside GRCD also because it is sitting in a common REV data. We had a similar challenge two three years back where BSMT team come and said that no you cannot implement like this that store under 170607 and then everybody use it. We had a very difficult conversation. Finally we told them it is up to the tech to implement it as long as everyone is abiding by the rule that every business is owning the data and every business is consuming it from the right place. We are not saying that we are not the AR or anything like that but it was a challenge from them. They said no go and implement this 10 times. not one time and then used by everybody. So those changes by the provider also by the different governance policies and all there is no clear but we have to just follow one simple thumb rule is technical implementation can be anything from the efficiency standpoint of view but that doesn't mean that every CSI is not the consumer of it. Every CSI should record them as a consumer of it.

Yeah. The other quick point I want to make sure we all know because I I was dealing this thing for one of Sanja's process right on the T area. Uh I want to make awareness here. So there is a system A which is SOR and it has let's say 2 CD field A and B. If system C gets them and they mark themsel to be AR okay they're they're free to do that with all the rules and regulation they could be AR of A and B because they're getting field A and field B from S. Okay. So now if the system C creates another field right AB equal to C system C becomes the SO for field C. Okay. Now the twist would be here someone gets data from system D as a file download and uploads it into system C. Right? In this case actually sor is D and can our system system C become the AR? So that was the there was a twist of the situation, right? And the rationale is like right now there is no integration automated integration between D and C some manual upload going on. So I I digged around like can system C become AR for field D and the answer was I was shocked. I thought the answer would be no because if someone manually uploading it that means there's an error and there is a possible use. see that some error could happen. So, system C should not be able to be the AR of of field D. The answer what I've got from the CDE office was shocking to me that it depends on who owns system C. If system C says I want to I want to take responsibility of the data control then system C could actually be AR of field D. So, in our case, let's say James Ferguson owns the system C. as a data officer, chief data officer, he it's his decision to say like he wants to be AR of field D or not but then he has to implement come to the tech team and put all the quality check etc here. So.

I mean that makes sense right so as long as they follow all the controls that are necessary to to become an AR assuming that they implemented all that it it does not matter whether you systematically consume assumed or whether you basically somebody has done the file file upload.

the the the control implementation may be different because in the in the in the system versus manual uploads.

yes.

but ultimately the control has to be there.

and that's a big responsibility so I just want to make sure if you guys obviously.

yeah if you guys have ever come into the situation the answer is yes even the manual upload you could be the AR however you have to have your business sign up and say like XYZ are covered. So I just wanted to bring the awareness because that was a challenging question that I'm allowing an EU to kind of you know file upload to become a AR or you know system of record basically somebody has to take the ownership.

uh before it's being sent downstream right so system C is saying okay I'll put the necessary controls I'll take the ownership and hence it becomes an AR so I think It makes sense actually.

Exactly.

Yeah. But again like I this was a question from other stream. I want to make sure that I I know I I let you guys know about that solution and the outcome that it is it is possible although it's in kind of promoting or allowing an EU to upload the data but it could still become AR being all those rule you know basis and controls in place. Okay. So anyway I end this call today here. I know that uh We have agenda for next week which is pretty full. Uh I think we'll need good for 80 minute and like everyone else please all the agenda items whatever you want to review put that on confluence page right it's uh you guys heard of heard from uh aim and you'll probably hear from Shinu and others Dan and others that put your agenda item that you want to discuss right in the confluence and Goro will share the link of the confluence to everyone on this call. Yeah,

before we end the call, I'll just tell about this data accuracy thing. One one one problems it it is worthwhile putting that kind of a check when we are processing it. So I'm not talking about the mandatory thing or not. Right? So I'm just talking as a practice to cover our um engineering practices 100%. Right? So I'll tell you the scenario. We consume the data from some front end uh some system they give a comma separated values in one of the feet that comma separated values could go maybe 6,000 characters in total over something when we brought that out it was not expected that we will have that many we coded it we designed it to consume the data and put into Oracle worker to field and that can take only 4,000 characters all of a sudden one particular record got 6,000 characters Oracle is supposed to fail. If somebody has implemented the code very intelligently, catch that exception but do not even basically fail that they wrote the partial record into the database and the whole process has become successful, right? It's possible process has been successful if at all you have coded it in a way that you caught the exception but you somehow did not stop the process but and then after Nobody no log now nobody knows that okay process was in error well and good like okay now your count checks are valid timeliness is valid but accuracy is not valid anymore right these are inner these issues could this be avoided yes if you properly have a graceful handling exception handling put it into the log put a monitoring alert there is an alert that needs to go out if If it was coded correctly, yes, everything is good. If somehow it's not coded correctly, you can run into issues. And in order to catch that kind of thing, we need to have a proper control whether in terms of our uh maybe our quality automation tools which can go and validate the stuff or maybe an external process that can go and do the check or maybe while while while processing the record itself, we have an intelligence built in to alert alert us. some implementation will have to be done but if people have not done correctly we can run into issue right so and and we expect business to find that out yes business will find out later agreed right hopefully hopefully business will find that out right among 1 million records they take a sample of like okay 10,000 records they validated it but what there is no guarantee that they're doing all 1 million records right?

yeah.

so such kind of very nuances like okay you never you never know things could go wrong.

yeah all right.

that's why I I support that one but it's complex problem like okay it adds up to our efforts.

yeah.

a lot.

so I'll get the clarity for once and for all that if we really need that or not right and I'll not take that on us it has to come from them but in a general tech practice I feel that we have a need of this external recon process no matter what right to as a offline process that independently verifies data in and data out. Yeah. All right. Thank you guys. Mangesh I do need your help man by the way since I see you here. Please look at the email that Shinu send because it is a report that goes to Jen Fraser for Genai. Uh it's like his uh her brag book. Um we need those information.

Okay, I'll have a look. Yeah,

please. Thank you. Thank you.

Thank you guys. Thank you.

Thank you. Sure.